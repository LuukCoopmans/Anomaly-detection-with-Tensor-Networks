{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with Tensor Networks (beta version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Luuk Coopmans, Trinity College Dublin and DIAS, luukcoopmans2@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to train tensor networks to perform anomaly detection on the MNIST dataset. We follow the paper [1] https://arxiv.org/abs/2006.02516 by J. Wang, C. Roberts, G. Vidal and S. Leichenauer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required libraries and modules (mostly updated versions as of April 12, 2021). We use the previously already existing libraries: jax for automatic differentation, tensorflow to import the datasets, sklearn for the score function and tensornetwork for tensor contractions. Moreover, some specific functions for the anomaly detection algorithm are included in the newly written ML_tensor module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update('jax_enable_x64', True) # enable 64-bit precision\n",
    "import jax.numpy as np\n",
    "from jax import grad, random, jit\n",
    "from jax.experimental import optimizers \n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"jax\")     # different backends are possible, see the tensornetwork documentation guide\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ML_tensor import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation of the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare the training and test data of the MNIST dataset (note other data sets such as the Fashion MNIST can be loaded and prepared as well). We redefine the trainingset to be the set with only one specific label and apply a (2x2) pooling layer to the images. For this we use the functions **one_label_subset** and **feature_map** from our ML_tensor module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_label = 1 # defining which MNIST label is a normal instance\n",
    "\n",
    "# loading and renormalizing the data\n",
    "mnist = tf.keras.datasets.mnist \n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# picking the subset of the data corresponding to the normal instance label\n",
    "x_train = one_label_subset(x_train, y_train, normal_label)\n",
    "\n",
    "# mapping the training data to input feature vectors suitable for our tensor network\n",
    "f_vecs = feature_map(x_train, Pooling=True)\n",
    "f_vecs_test = feature_map(x_test, Pooling=True)\n",
    "\n",
    "# Making the test label vector (1 for normal_instance and 0 for anomaly)\n",
    "y_test[np.where(y_test==normal_label)] = 1\n",
    "y_test[np.where(y_test!=normal_label)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check to see that we have selected the right data we can plot some of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[3],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training in batches we can split up the feature vectors (f_vecs) into smaller batches with tensorflows split function. Note that we cut of the number off f_vecs such that we have an integer number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  32\n",
    "\n",
    "n = int(np.floor(len(f_vecs)/batch_size)) # integer number of batches\n",
    "batches = np.array(tf.split(f_vecs[0:batch_size*n], n))\n",
    "\n",
    "print(np.shape(batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The trainable tensor network (matrix product operator), loss function and penalty function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central object we are going to train for anomaly detection is a tensor network (more specifically a linear transformation known as matrix product operator MPO see [1] for details). The MPO is nothing but a nested list of numpy arrays which contain our trainable parameters. The indices of the arrays correspond the the legs of the tensor network and can be dangling or bonded. Dangling means they don't connect with the neighbouring arrays in the list whereas the bonded indices do. Increasing the dimension of these indices will lead to more trainable parameters. To initialize a random MPO we can just call the **rand_anomdet_MPO** from the ML_tensor module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # dimension of the input legs (should match the dimension axis=1 of the f_vecs)\n",
    "b = 5 # dimension of the bonds\n",
    "p = 2 # dimension of the output legs\n",
    "S = 8 # parameter that determines the number of output legs.\n",
    "\n",
    "mean = 0\n",
    "std = 0.31\n",
    "\n",
    "MPO = rand_anomdet_MPO(np.shape(f_vecs)[2],d, b, p, S, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the MPO to a single feature vector (required for the loss function, see below) we can call the **apply_MPO_to_fvec** function (first time maybe slow to run due to jit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvec = f_vecs[0]\n",
    "MPS = apply_MPO_to_fvec(fvec, MPO) # we call this a matrix product state (MPS) because of the form of the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision function for the anomaly detection is defined to be the squared F (Frobenius) norm of the linear transformation (MPO $\\equiv P$) applied to a feature vector $||P(f_{vec})||^2_2$. It returns a 1 for a normal instance and a 0 for an anomaly based on the radius $\\epsilon$ known as the decision boundary which we set manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.5 # decision radius\n",
    "print('Decision for fvec is:',decision_fun(eps, MPO, fvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training we define the loss of a single f_vec to be the log of the squared F-norm -1 all squared, i.e. $(log(||P(f_{vec})||^2_2)-1)^2$. We can compute this loss by calling the function **loss_function** from ML_tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(MPO, fvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid converging to a trivial solution we add a penalty term to every batch of losses given by the rectified linear of the log of the squared F operator norm: ReLu($||P||^2_2$). This penalty can be computed with the **penalty** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(penalty(MPO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradients and the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the gradient of the individual loss and penalty functions we make use of the grad function from the jax library. This function takes in a function and returns the corresponding gradient function which can be called with the same arguments. Note that running the cells below for the first time could take a while due the compilation of the jit, running the cell for the second time is should be must faster thanks to the jit (see jax documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_gradient = jit(grad(loss_function))\n",
    "loss_grad = loss_gradient(MPO,fvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_gradient = jit(grad(penalty))\n",
    "grad_pen = penalty_gradient(MPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a slow jit compilation (could take a few minutes) it should run now fast (milliseconds) in the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = loss_gradient(MPO,fvec)\n",
    "grad_pen = penalty_gradient(MPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the values and gradient for a single batch we can call the **batch_loss_and_gradient** again due to jit the first time this can be slow (up to 5 min). The hyperparameter alpha is the relative importance of the penalty versus the loss that is chosen manually as in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "batch = batches[0]\n",
    "value, pen_value, grads = batch_loss_and_gradient(MPO,batch, alpha)\n",
    "print('Loss is:',value, 'Penalty is',pen_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the predictions of the MPO anomaly detector with the **anomaly_detection** function and compute the roc_auc score (from sklearn): (again this is a bit slow unfortunately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = anomaly_detection(MPO,f_vecs_test)\n",
    "print('Starting Roc_auc score is:',roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define the learning loop we can initialize an optimizer for the optimization by using calling one from the jax library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size) # other optimizers such as SGD are also available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are now ready to set up the training loop and start training our tensor network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =  5\n",
    "print_roc_every = 5\n",
    "\n",
    "loss_list = []\n",
    "pen_list = []\n",
    "\n",
    "opt_state = opt_init(MPO) # initialize the learnable parameters for the optimizer\n",
    "\n",
    "value, pen_value, grads = batch_loss_and_gradient(MPO,batch, 0.4) # initial value and gradient\n",
    "loss_list.append(value)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    for j in range(len(batches)):\n",
    "        \n",
    "        batch = batches[j]\n",
    "        \n",
    "        # update the MPO\n",
    "        value, pen_value, grads = batch_loss_and_gradient(MPO,batch, alpha)\n",
    "        opt_state = opt_update(i, grads, opt_state)\n",
    "        MPO = get_params(opt_state)\n",
    "        \n",
    "        if j%10 == 0:\n",
    "            print('After batch update step:',j,'new loss is:', value, 'new penalty is:', pen_value)\n",
    "        \n",
    "    # update learning curves every epoch\n",
    "    loss_list.append(value)\n",
    "    pen_list.append(pen_value)\n",
    "    \n",
    "    # compute and print roc_au score every print_roc_every epochs\n",
    "    if i%print_roc_every == 0:\n",
    "        y_pred = anomaly_detection(MPO,f_vecs_test)\n",
    "        print('New roc_auc score is:',roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    # save the MPO every epoch\n",
    "    with open('MPO_epoch{}.pkl'.format(i),'wb') as f:\n",
    "            pickle.dump(MPO, f)\n",
    "\n",
    "# save the final learning curve\n",
    "np.save('loss_array.npy',np.array(loss_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the learning and penalty curves to watch the progress of the learning algorithm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(loss_list))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(pen_list))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('penalty value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can compute the final predictions and roc_auc score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = anomaly_detection(MPO,f_vecs_test)\n",
    "print('Roc_auc score is:',roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Notes for future updates/improvements of the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are countless possibilities to improve the above code. Most importantly the computational time should be improved. Currently the code is only running on one core (the exact reason why this is happening is at this point still a bit unclear to me). I expect that once it runs on a few cpu cores or even on a gpu it will be much faster. Another bottleneck in time I discovered is in the gradient update of the MPO: while running the **batch_loss_and_gradient** (the 2nd time after jit compilation) for the full epoch size takes about 33 seconds within the training loop it takes 176 seconds on my laptop (see test below).    \n",
    "\n",
    "- When the computational time is reduced it will be also possible to scan over the hyper parameters $\\alpha$, batch_size and step_size to determine the optimal learning settings. Currently the algorithm converges to a roc_auc_score of about 0.85 after 5 epochs. In the paper [1] 300 episodes were used and a roc_auc score of 0.998 was found. I expect that once the optimal learning parameters are set the value of the roc_auc score of the code presented here should become closer to the score in [1].\n",
    "\n",
    "- Another point worth investigating further is the instability of the penalty function, it can get very big due to the contraction of the many tensors in the MPO (this was also reported in [1]). Perhaps with different initial conditions or a different value for alpha this could be resolved. Another option would be to look if it is possible  to normalize the MPO in some way (eg this can be done for standard MPS states).  \n",
    "\n",
    "- (Technical point) Finally the feature map used here maps images to product states, in future releases it might be interesting to explore other ways of encoding the images such as in entangled states. An example I am thinking of is that you take a subset of images (product states) add them together in an approximate fashion (ie keeping the bond dimension low) which forms a new entangled MPS. Then this new entangled MPS could be again combined with an MPO like done here. Or it could be interesting to compute the overlap of this MPS with other states within and outside the training set to see if you can use it as an anomaly detector (and/or classifier). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for questions and suggestions please email me at luukcoopmans2@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the time of an epoch (without update):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "then = time.time()\n",
    "for i in range(len(batches)):\n",
    "    value, pen_value, grads = batch_loss_and_gradient(MPO,batch, alpha)\n",
    "    print(time.time()-then)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the time of an epoch (with adam updates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size) # other optimizers such as SGD are also available\n",
    "opt_state = opt_init(MPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "then = time.time()\n",
    "for i in range(len(batches)):\n",
    "    value, pen_value, grads = batch_loss_and_gradient(MPO,batch, alpha)\n",
    "    opt_state = opt_update(i, grads, opt_state)\n",
    "    MPO = get_params(opt_state)\n",
    "    print(time.time()-then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "380fb6f2b8854f8ebf505215c3e956a8",
   "lastKernelId": "e8bd3566-56e7-4725-9c35-a91e6b065bd1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
